{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f39117-48a7-4c24-8e7d-7a7c561bb53e",
   "metadata": {},
   "source": [
    "# Init & Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021216f0-751d-42c6-a049-0166f0eea696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, pickle, os\n",
    "now = time.time\n",
    "from math import ceil\n",
    "from random import randrange, choice, random\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## Client Info ##\n",
    "CLIENT_ID     = \"\"\n",
    "CLIENT_SECRET = \"\"\n",
    "CLIENT_SCOPE  = \"user-follow-modify playlist-modify-private playlist-modify-public\"\n",
    "USER_NAME     = \"31ytgsr7wdmiaroy77msqpiupdsi\"\n",
    "REDIR_URI     = \"https://github.com/jwatson-CO-edu/yt_shuffle_so_good\"\n",
    "AUTH_URL      = 'https://accounts.spotify.com/api/token'\n",
    "BASE_URL      = 'https://api.spotify.com/v1/'\n",
    "## API Info ##\n",
    "_RESPONSE_LIMIT =  100\n",
    "_MAX_OFFSET     = 1000\n",
    "_T_LOGIN_S      = 45 * 60.0\n",
    "tLastAuth       = 0.0\n",
    "\n",
    "with open( \"../keys/spot_ID.txt\" , 'r' ) as f:\n",
    "    CLIENT_ID = f.readlines()[0].strip()\n",
    "\n",
    "with open( \"../keys/spot_SECRET.txt\" , 'r' ) as f:\n",
    "    CLIENT_SECRET = f.readlines()[0].strip()\n",
    "\n",
    "token = None\n",
    "spot  = None\n",
    "\n",
    "\n",
    "def check_API_token():\n",
    "    global tLastAuth, token, _T_LOGIN_S, spot\n",
    "    tNow    = now()\n",
    "    elapsed = tNow - tLastAuth\n",
    "    if elapsed >= _T_LOGIN_S:\n",
    "        token = util.prompt_for_user_token(\n",
    "            username      = USER_NAME,\n",
    "            scope         = CLIENT_SCOPE,\n",
    "            client_id     = CLIENT_ID,\n",
    "            client_secret = CLIENT_SECRET,\n",
    "            redirect_uri  = REDIR_URI\n",
    "        )\n",
    "        spot = spotipy.Spotify( auth = token )\n",
    "        print( token )\n",
    "        clear_output( wait = True )\n",
    "        sleep( 2 )\n",
    "        print( \"TOKEN OBTAINED\" )\n",
    "        tLastAuth = tNow\n",
    "    else:\n",
    "        print( f\"TOKEN STILL VALID, AGE: {elapsed/60.0} MINUTES\" )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643e4861-2fed-44bf-81d0-53af493d8f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN OBTAINED\n"
     ]
    }
   ],
   "source": [
    "check_API_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d1583-abb2-4c30-af15-f43082b839ee",
   "metadata": {},
   "source": [
    "## Playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ad3b37-1ec4-474d-adfc-fcad78f6f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = {\n",
    "    'study01' : \"0a2qoe6S7lYeZ6nlhZdA0v\",\n",
    "    'study02' : \"6gbtR2cBq5PvkghidCvvGk\",\n",
    "    'study03' : \"3o3lN2qntdEV7UKTuuC77K\",\n",
    "    'study04' : \"41sFSisljvBDMBXtpp5NIw\",\n",
    "    'study05' : \"02iS5AFGp8YVuUUqcQf8ys\",\n",
    "    'study06' : \"6KI7A4MWrSM7EyKRUjxIi1\",\n",
    "    'study07' : \"3V055Md2JdrUT8tX0af7di\",\n",
    "    'study08' : \"0tspdJlwSgiyf2O9PO6QaP\",\n",
    "    'study09' : \"5mHRBFoQtYy2izeZ66pG95\",\n",
    "    'study10' : \"3832xeKGEOAXFJqE4K8kIq\",\n",
    "    'study11' : \"65MXR4dubPL9t0P4dgTWvn\",\n",
    "    'study12' : \"0ecSAfnD4CulIVnLt26ukI\",\n",
    "    'study13' : \"7K9ucByFRgDuZk8KMHeJkL\",\n",
    "}\n",
    "\n",
    "review = {\n",
    "    'zd_Over' : \"0v26bHydUxcGC5EbMlkjzG\",\n",
    "    'ze_Over' : \"6SqlfurCBP7eeMOojaDNtS\",\n",
    "    'zf_Over' : \"5TtKaKCouyJp7Hhtu4YlYm\",\n",
    "}\n",
    "\n",
    "backfill = review['zd_Over']\n",
    "_N_BKFL  = 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857e3ff-a087-4b18-933b-58398c97b3c6",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb557d50-fe23-44d5-b919-65d3f68ae5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FILTER_TYPES = [ 'album', 'artist', 'track', 'year', 'upc', 'tag:hipster', 'tag:new', 'isrc', 'genre', ]\n",
    "_SEARCH_TYPES = [ \"album\", \"artist\", \"playlist\", \"track\", \"show\", \"episode\", \"audiobook\", ]\n",
    "_N_MAX_SEARCH = 50\n",
    "_N_DEF_SEARCH = 10\n",
    "_YEAR_PADDING =  5\n",
    "_MOD_T_DAY_S  = 60.0 * 60 * 24\n",
    "_STALE_TIME_S = _MOD_T_DAY_S * 31\n",
    "_MIN_LEN_S    = 60.0 + 45.0\n",
    "_DATA_DIR     = \"data/\"\n",
    "_DATA_PREFIX  = \"Study-Music-Data_\"\n",
    "_DATA_POSTFIX = \".pkl\"\n",
    "_NULL_GENRE   = \"Music\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13120a9-81f1-4e7a-a86e-ee7112d4247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'time'     : now()  , # Data Structure Creation Time\n",
    "    'playlists': dict() , # Study Playlist Info\n",
    "    'collectID': set([]), # Currently accepted track IDs\n",
    "    'review'   : dict() , # Review Playlist Info\n",
    "    'reviewID' : set([]), # Previously reviewed track IDs\n",
    "    'artists'  : dict() , # Study Artist Info\n",
    "    'queries'  : dict() , # Queries made during music searches\n",
    "    'genres'   : dict() , # Study Genre Info\n",
    "    # 2024-08-11: Track info does NOT contain play count\n",
    "}\n",
    "timestamp = datetime.now().strftime( '%Y-%m-%dT%H:%M:%S' )\n",
    "outFilNam = _DATA_PREFIX + timestamp + _DATA_POSTFIX\n",
    "outPath   = os.path.join( 'data/', outFilNam )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5d090-4c12-4c71-b203-db3f04ff3aaf",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ed8f73-8f9e-42d4-bac8-a346ac74c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_SKIP_GENRE_BUILD = False\n",
    "_SKIP_GENRE_MERGE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ffb0b-076d-4a9b-b61b-3edd6eaf453d",
   "metadata": {},
   "source": [
    "# Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301c30f0-0f7a-4b1b-9a0f-b96a26a2c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_entire_playlist( playlist_ID ):\n",
    "    \"\"\" Get infodump on all plalist tracks \"\"\"\n",
    "    plTracks = []\n",
    "    trCount  = 0\n",
    "    response = spot.user_playlist_tracks(\n",
    "        CLIENT_ID, \n",
    "        playlist_ID, \n",
    "        fields = 'items,uri,name,id,total', \n",
    "        limit  = _RESPONSE_LIMIT\n",
    "    )\n",
    "    Ntracks = response['total']\n",
    "    while 1:\n",
    "        trCount += len(response['items'])\n",
    "        plTracks.extend( response['items'] )\n",
    "        \n",
    "        if trCount >= Ntracks:\n",
    "            break\n",
    "    \n",
    "        response = spot.user_playlist_tracks(\n",
    "            CLIENT_ID, \n",
    "            playlist_ID, \n",
    "            fields = 'items,uri,name,id,total', \n",
    "            limit  = _RESPONSE_LIMIT,\n",
    "            offset = trCount\n",
    "        )\n",
    "    return plTracks\n",
    "\n",
    "\n",
    "def load_music_database( dataDir = _DATA_DIR, forceLoad = False ):\n",
    "    \"\"\" Find the latest music database, test for freshness, and set current db if fresh \"\"\"\n",
    "    global data\n",
    "    dbFiles = [os.path.join( dataDir, f ) for f in os.listdir( dataDir ) if (_DATA_PREFIX in str(f))]\n",
    "    if len( dbFiles ):\n",
    "        dbFiles.sort( reverse = True )\n",
    "        with open( dbFiles[0], 'rb' ) as f:\n",
    "            db = pickle.load( f )\n",
    "        if (((data['time'] - db['time']) <= _STALE_TIME_S) or forceLoad):\n",
    "            data.update( db )\n",
    "            print( f\"Loaded {dbFiles[0]}!\" )\n",
    "            return dbFiles[0]\n",
    "        else:\n",
    "            print( f\"File {dbFiles[0]} was STALE by {(data['time']-db['time']-_STALE_TIME_S)/_MOD_T_DAY_S} days!\" )\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "def populate_playlist_data( dataDct, plDict, pause_s = 1.0 ):\n",
    "    \"\"\" Gather data across specified playlists \"\"\"\n",
    "    print( \"\\n### READ MUSIC COLLECTION ###\\n\" )\n",
    "    nuDB = load_music_database()\n",
    "    if nuDB is not None:\n",
    "        print( f\"Found current collection data at {nuDB}!\" )\n",
    "    else:\n",
    "        for plName_i, plID_i in plDict.items():\n",
    "            print( plName_i, '-', plID_i, '...' )\n",
    "            dataDct['playlists'][ plName_i ] = {\n",
    "                'ID'    : plID_i,\n",
    "                'tracks': fetch_entire_playlist( plID_i ),\n",
    "            }\n",
    "            # pprint( dataDct['playlists'][ plName_i ]['tracks'][0] )\n",
    "            # return None\n",
    "            plSet_i = set([item['track']['id'] for item in dataDct['playlists'][ plName_i ]['tracks']])\n",
    "            dataDct['collectID'] = dataDct['collectID'].union( plSet_i )\n",
    "    \n",
    "            for track_j in dataDct['playlists'][ plName_i ]['tracks']:\n",
    "    \n",
    "                # pprint( track_j )\n",
    "                for artist_k in track_j['track']['artists']:\n",
    "                    artistID_j = artist_k['id']\n",
    "                    if artistID_j not in dataDct['artists']:\n",
    "                        dataDct['artists'][ artistID_j ] = { \n",
    "                            'name'    : track_j['track']['artists'][0]['name'], \n",
    "                            'count'   : 1, \n",
    "                            'releases': [track_j['track']['album']['release_date'],], \n",
    "                        }\n",
    "                    else:\n",
    "                        dataDct['artists'][ artistID_j ]['count'   ] += 1\n",
    "                        dataDct['artists'][ artistID_j ]['releases'].append( track_j['track']['album']['release_date'] )\n",
    "    \n",
    "            sleep( pause_s )\n",
    "    \n",
    "    print( \"\\n### COMPLETE ###\\n\" )\n",
    "    \n",
    "\n",
    "def search_artist_within_era( artistName, releaseDate, \n",
    "                              db = None, N = _N_MAX_SEARCH, yearPadding = _YEAR_PADDING, pause_s = 0.5 ):\n",
    "    \"\"\" Return `N` tracks within `yearPadding` of `trackDict` and by the same artist \"\"\"\n",
    "    rtnLs = list()\n",
    "    query = \"artist%3A\" + str( artistName ).replace( \" \", \"%20\")\n",
    "    try:\n",
    "        rYear = int( str( releaseDate )[:4] )\n",
    "    except Exception:\n",
    "        rYear = 2024\n",
    "    bYear   = rYear - yearPadding\n",
    "    eYear   = rYear + yearPadding\n",
    "    years   = list( range( bYear, eYear+1 ) )\n",
    "    miniLim = max( int(N/(eYear - bYear)), 1 )\n",
    "    Nloop   = int(N / miniLim * 2)\n",
    "    for i in range( Nloop ):\n",
    "        iYear = choice( years )\n",
    "        qry_i = query + \"%20year%3A\" + str( iYear )\n",
    "        print( f\"Search: {qry_i}\" )\n",
    "\n",
    "        if (db is not None):\n",
    "            if (qry_i in db['queries']):\n",
    "                ofst = db['queries'][ qry_i ]\n",
    "                db['queries'][ qry_i ] += miniLim\n",
    "            else:\n",
    "                ofst = 0\n",
    "                db['queries'][ qry_i ] = miniLim\n",
    "        else:\n",
    "            ofst = 0\n",
    "        \n",
    "        res = spot.search( qry_i, \n",
    "                           limit  = miniLim, \n",
    "                           offset = min( ofst, _MAX_OFFSET ), \n",
    "                           type   = 'track' )\n",
    "        tracks_i = [item['id'] for item in res['tracks']['items']]\n",
    "        \n",
    "        if (db is not None):\n",
    "            tracks_ii = list()\n",
    "            for trk_j in tracks_i:\n",
    "                if trk_j not in db['collectID']:\n",
    "                    tracks_ii.append( trk_j )\n",
    "            tracks_i = tracks_ii[:]\n",
    "\n",
    "        rem = N - len( rtnLs )\n",
    "        if len( tracks_i ) > rem:\n",
    "            rtnLs.extend( tracks_i[:rem] )\n",
    "            return rtnLs\n",
    "        else:\n",
    "            rtnLs.extend( tracks_i )\n",
    "            sleep( pause_s )\n",
    "    return rtnLs\n",
    "\n",
    "\n",
    "def save_music_database( dataDct ):\n",
    "    \"\"\" Pickle `dataDct` to store current music collection data as well as search activity \"\"\"\n",
    "    print( f\"About to write {outPath} ...\" )\n",
    "    with open( outPath, 'wb' ) as f:\n",
    "        pickle.dump( dataDct, f )\n",
    "    print( \"COMPLETE!\" )\n",
    "\n",
    "\n",
    "def choose_N_artist_year_pairs_from_db( N, db ):\n",
    "    \"\"\" Fetch `N` random (<Artist>, <Date>) pairs from the `db` for searching \"\"\"\n",
    "    rtnPairs = list()\n",
    "    artList  = list( db['artists'].keys() )\n",
    "    for i in range(N):\n",
    "        artKey_i = choice( artList )\n",
    "        # pprint( db['artists'][ artKey_i ] )\n",
    "        artist_i = db['artists'][ artKey_i ]['name']\n",
    "        rlYear_i = choice( db['artists'][ artKey_i ]['releases'] )\n",
    "        rtnPairs.append( (artist_i, rlYear_i,) )\n",
    "    return rtnPairs\n",
    "\n",
    "\n",
    "def basic_new_music_search_01( db, Ntot, Mper = 5, pause_s = 0.125 ):\n",
    "    \"\"\" Choose random `db` entries as search queries, Return a list of `Ntot` tracks consisting of `Mper` entries for each artist \"\"\"  \n",
    "    rtnLst   = list()\n",
    "    searches = choose_N_artist_year_pairs_from_db( int( ceil( Ntot/Mper ) )*2, db )\n",
    "    addSet   = set([])\n",
    "    for (art_i, rel_i) in searches:\n",
    "        print( f\"\\tSearch, Artist: {art_i}, Around Year: {rel_i}\" )\n",
    "        rem    = Ntot - len( rtnLst )\n",
    "        Mper   = min( Mper, rem )\n",
    "        trks   = search_artist_within_era( art_i, rel_i, db, N = Mper, yearPadding = 3, pause_s = 0.5 )\n",
    "        trks_i = list()\n",
    "        for trk in trks:\n",
    "            if trk not in addSet:\n",
    "                trks_i.append( trk )\n",
    "                addSet.add( trk )\n",
    "        rem = Ntot - len( rtnLst )\n",
    "        if rem > len( trks_i ):\n",
    "            rtnLst.extend( trks_i )\n",
    "        else:\n",
    "            rtnLst.extend( trks_i[ :rem ] )\n",
    "            break\n",
    "        sleep( pause_s )\n",
    "    return rtnLst\n",
    "\n",
    "\n",
    "def get_playlist_length( playlist_ID ):\n",
    "    \"\"\" Get the number of total tracks in the playlist \"\"\"\n",
    "    response = spot.user_playlist_tracks(\n",
    "        CLIENT_ID, \n",
    "        playlist_ID, \n",
    "        fields = 'items,uri,name,id,total', \n",
    "        limit  = _RESPONSE_LIMIT\n",
    "    )\n",
    "    return response['total']\n",
    "\n",
    "\n",
    "def refill_playlist_with_new_tracks( plID, db, Ntot = 400, Mper = 5 ):\n",
    "    \"\"\" Top off the playlist with new tracks \"\"\"\n",
    "    plLen = get_playlist_length( plID )\n",
    "    if Ntot > plLen:\n",
    "        nRem    = Ntot - plLen\n",
    "        print( f\"About to add {nRem} tracks ...\" )\n",
    "        addTrks = basic_new_music_search_01( db, nRem, Mper )\n",
    "        result  = spot.user_playlist_add_tracks( CLIENT_ID, plID, addTrks )\n",
    "        print( result )\n",
    "    else:\n",
    "        print( \"No room for new tracks!\" )\n",
    "\n",
    "\n",
    "def scrub_short_and_explicit_tracks( plID ):\n",
    "    \"\"\" Remove short (<1:45) and explicit (guaranteed vocal) songs \"\"\"\n",
    "    tracks = fetch_entire_playlist( plID )\n",
    "\n",
    "    # 3. For every track j in playlist, do\n",
    "    j = 0\n",
    "    while j < len( tracks ):\n",
    "        track_j   = tracks[j]\n",
    "        # pprint( track_j )\n",
    "        # break\n",
    "        p_dump_j  = False\n",
    "        trackID_j = track_j['track']['id']\n",
    "        len_s_j   = track_j['track']['duration_ms']/1000.0\n",
    "        explc_j   = track_j['track']['explicit']\n",
    "\n",
    "        if ((len_s_j < _MIN_LEN_S) or explc_j):\n",
    "            res = spot.playlist_remove_specific_occurrences_of_items( \n",
    "                plID, \n",
    "                [{'uri': trackID_j, 'positions':[j,]},]\n",
    "            )\n",
    "            print( \"\\tRemove:\", trackID_j, j, res )\n",
    "            tracks.pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "\n",
    "def scrub_and_refill_playlist_with_new_tracks( plID, db, Ntot = 400, Mper = 5, pause_s = 1.0 ):\n",
    "    \"\"\" Remove suspect tracks and refill backfill in a loop until full \"\"\"\n",
    "    scrub_short_and_explicit_tracks( plID )\n",
    "    plLen = get_playlist_length( plID )\n",
    "    if Ntot > plLen:\n",
    "        print( f\"\\n########## About to top off backfill with {Ntot-plLen} tracks! ##########\\n\" )\n",
    "        i = 0\n",
    "        while plLen < Ntot:\n",
    "            i += 1\n",
    "            sleep( pause_s )\n",
    "            print( f\"\\n##### Iteration {i} #####\\n\" )\n",
    "            refill_playlist_with_new_tracks( plID, db, Ntot, Mper )\n",
    "            scrub_short_and_explicit_tracks( plID )\n",
    "            plLen = get_playlist_length( plID )\n",
    "            \n",
    "        print( f\"\\n########## COMPLETE after {i} iterations! ##########\\n\" )\n",
    "    else:\n",
    "        print( f\"\\n########## NO additional backfill required! ##########\\n\" )\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38233c1a-2593-4307-bd16-c894333fae71",
   "metadata": {},
   "source": [
    "# Search Version 01, Query by Existing Artist and Year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7ed15-d065-47b7-8d91-01db7b17831b",
   "metadata": {},
   "source": [
    "## Read Study Music Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c775a4b-47c3-45f0-a0e9-10f5ffbbc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate_playlist_data( data, playlist, pause_s = 1.0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e57930-b224-42dd-9bcf-de1fe6b5e5a0",
   "metadata": {},
   "source": [
    "## Add New Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22be8b27-8e1c-4882-9d66-a28aacd64775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refill_playlist_with_new_tracks( backfill, data, Ntot = _N_BKFL, Mper = 5 )\n",
    "# scrub_and_refill_playlist_with_new_tracks( backfill, data, Ntot = 400, Mper = 5, pause_s = 1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f25401a1-5b2c-4144-9f8d-1b80e951380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_music_database( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da9dcce0-3f36-4fef-8b83-73c7f29a9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrub_short_and_explicit_tracks( backfill )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f26f5-5906-409a-9093-eb48d1c17156",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab8a170-ba6c-41a4-a851-7bdb13d07bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## CONTAINER FUNCTIONS #####################################################################\n",
    "\n",
    "def sort_keys_by_value( dct, reverse = True ):\n",
    "    \"\"\" Return a list of keys sorted by their (numeric) values \"\"\"\n",
    "    srtLst = list()\n",
    "    for k, v in dct.items():\n",
    "        srtLst.append( [v,k,] )\n",
    "    srtLst.sort( key = lambda x: x[0], reverse = reverse )\n",
    "    return [pair[1] for pair in srtLst] \n",
    "\n",
    "\n",
    "\n",
    "########## STRING ANALYSIS #########################################################################\n",
    "\n",
    "def levenshtein_dist( s1, s2 ):\n",
    "    \"\"\" Get the edit distance between two strings \"\"\"\n",
    "    # Author: Salvador Dali, https://stackoverflow.com/a/32558749\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "\n",
    "\n",
    "########## STATS & SAMPLING ########################################################################\n",
    "\n",
    "\n",
    "def total_pop( odds ):\n",
    "    \"\"\" Sum over all categories in the prior odds \"\"\"\n",
    "    total = 0\n",
    "    for k in odds:\n",
    "        total += odds[k]\n",
    "    return total\n",
    "\n",
    "\n",
    "def normalize_dist( odds_ ):\n",
    "    \"\"\" Normalize the distribution so that the sum equals 1.0 \"\"\"\n",
    "    total  = total_pop( odds_ )\n",
    "    rtnDst = dict()\n",
    "    for k in odds_:\n",
    "        rtnDst[k] = odds_[k] / total\n",
    "    return rtnDst\n",
    "\n",
    "\n",
    "def roll_outcome( odds ):\n",
    "    \"\"\" Get a random outcome from the distribution \"\"\"\n",
    "    oddsNorm = normalize_dist( odds )\n",
    "    distrib  = []\n",
    "    outcome  = []\n",
    "    total    = 0.0\n",
    "    for o, p in oddsNorm.items():\n",
    "        total += p\n",
    "        distrib.append( total )\n",
    "        outcome.append( o )\n",
    "    roll = random()\n",
    "    for i, p in enumerate( distrib ):\n",
    "        if roll <= p:\n",
    "            return outcome[i]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c7cc5-3ed3-4d66-892a-5e7199864955",
   "metadata": {},
   "source": [
    "# Micro-Genre Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee84d7bf-784a-4e06-a2b8-a5ee7ca2b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "import enchant\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "_DBS_EPSILON  =  0.75\n",
    "_DBS_MIN_MMBR =  5\n",
    "_ARTIST_Q_LIM = 50\n",
    "\n",
    "\n",
    "def fetch_entire_playlist_with_audio_features( playlist_ID ):\n",
    "    \"\"\" Get maximum infodump on all playlist tracks \"\"\"\n",
    "    plTracks = []\n",
    "    trCount  = 0\n",
    "    \n",
    "    response = spot.user_playlist_tracks(\n",
    "        CLIENT_ID, \n",
    "        playlist_ID, \n",
    "        fields = 'items,uri,name,id,total', \n",
    "        limit  = _RESPONSE_LIMIT\n",
    "    )\n",
    "    resTracks = response['items']\n",
    "    Ntracks   = response['total']\n",
    "\n",
    "    resIDs = [item['track']['id'] for item in resTracks]\n",
    "\n",
    "    resFeatrs = spot.audio_features( resIDs )\n",
    "    for i, track_i in enumerate( resTracks ):\n",
    "        track_i.update( resFeatrs[i] )\n",
    "    \n",
    "    while 1:\n",
    "        trCount += len( resTracks )\n",
    "        plTracks.extend( resTracks )\n",
    "        \n",
    "        if trCount >= Ntracks:\n",
    "            break\n",
    "    \n",
    "        response = spot.user_playlist_tracks(\n",
    "            CLIENT_ID, \n",
    "            playlist_ID, \n",
    "            fields = 'items,uri,name,id,total', \n",
    "            limit  = _RESPONSE_LIMIT\n",
    "        )\n",
    "        resTracks = response['items']\n",
    "    \n",
    "        resIDs = [item['track']['id'] for item in resTracks]\n",
    "    \n",
    "        resFeatrs = spot.audio_features( resIDs )\n",
    "        for i, track_i in enumerate( resTracks ):\n",
    "            track_i.update( resFeatrs[i] )\n",
    "    return plTracks\n",
    "    \n",
    "\n",
    "_V_NUM_FEATURES  = 10\n",
    "\n",
    "_V_SPEECH_FACTOR =  1.0 / 0.3724 * 3.0\n",
    "_V_INSTR_FACTOR  =  1.0 * 3.0\n",
    "_V_ACOUST_FACTOR =  1.0\n",
    "_V_DANCE_FACTOR  =  1.0 /  0.8701\n",
    "_V_DURATN_FACTOR =  1.0 / 1000.0 / _MIN_LEN_S / 18.7821619\n",
    "_V_ENERGY_FACTOR =  1.0\n",
    "_V_LIVENS_FACTOR =  1.0 /   0.9173\n",
    "_V_LOUDNS_FACTOR =  1.0 /  38.53\n",
    "_V_TEMPO_FACTOR  =  1.0 / (174.331-45.7)\n",
    "_V_VALENC_FACTOR =  1.0\n",
    "\n",
    "\n",
    "def get_track_vector( track ):\n",
    "    \"\"\" Express the track characteristics as a vector \"\"\"\n",
    "    return np.array([\n",
    "        track['speechiness'] * _V_SPEECH_FACTOR,\n",
    "        track['instrumentalness'] * _V_INSTR_FACTOR,\n",
    "        track['acousticness'] * _V_ACOUST_FACTOR,\n",
    "        track['danceability'] * _V_DANCE_FACTOR,\n",
    "        track['duration_ms'] * _V_DURATN_FACTOR,\n",
    "        track['energy'] * _V_ENERGY_FACTOR,\n",
    "        track['liveness'] * _V_LIVENS_FACTOR,\n",
    "        track['loudness'] * _V_LOUDNS_FACTOR,\n",
    "        (track['tempo']-45.7) * _V_TEMPO_FACTOR,\n",
    "        track['valence'] * _V_VALENC_FACTOR,\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_tracks_as_vectors( tracks ):\n",
    "    \"\"\" Convert all tracks to vectors \"\"\"\n",
    "    Mrows  = len( tracks )\n",
    "    if Mrows > 0:\n",
    "        Ncols  = len( get_track_vector( tracks[0] ) )\n",
    "        rtnMtx = np.zeros( (Mrows, Ncols,) ) \n",
    "        for i, trk in enumerate( tracks ):\n",
    "            rtnMtx[i,:] = get_track_vector( trk )\n",
    "        return rtnMtx\n",
    "    else:\n",
    "        return list()\n",
    "\n",
    "\n",
    "def vector_distance_to_genre( qVec, genreDct ):\n",
    "    \"\"\" Get Euclidean distance between `qVec` and the nearest track vector of `genreDct` \"\"\"\n",
    "    return genreDct['kdTree'].query( qVec )[0]\n",
    "    \n",
    "\n",
    "def track_distance_to_genre( qTrack, genreDct ):\n",
    "    \"\"\" Get Euclidean distance between `qTrack` and the nearest track vector of `genreDct` \"\"\"\n",
    "    return vector_distance_to_genre( get_track_vector( qTrack ), genreDct )\n",
    "\n",
    "\n",
    "def assign_vectors_to_tracks( tracks ):\n",
    "    \"\"\" Store vector info in each track dictionary \"\"\"\n",
    "    matxTrks = get_tracks_as_vectors( tracks )\n",
    "    for i, vec_i in enumerate( matxTrks ):\n",
    "        trk_i = tracks[i]\n",
    "        trk_i['vector'] = vec_i\n",
    "    print( f\"Stored vectors for {len(tracks)} tracks!\" )\n",
    "\n",
    "\n",
    "def fetch_collection_with_audio_features( dataDct, plDct, rvDct = None, pause_s = 3.0, renewSets = True ):\n",
    "    \"\"\" Get maximum infodump on all playlists \"\"\"\n",
    "\n",
    "    if renewSets:\n",
    "        dataDct['collectID'] = set([])\n",
    "        \n",
    "    \n",
    "    print( \"##### Get Collection Data #####\" )\n",
    "    for plName_i, plID_i in plDct.items():\n",
    "        print( plName_i, '-', plID_i, '...' )\n",
    "        tracks_i = fetch_entire_playlist_with_audio_features( plID_i )\n",
    "        assign_vectors_to_tracks( tracks_i )\n",
    "        dataDct['playlists'][ plName_i ] = {\n",
    "            'ID'    : plID_i,\n",
    "            'tracks': tracks_i,\n",
    "            'len'   : len( tracks_i ),\n",
    "        }\n",
    "        dataDct['collectID'] = dataDct['collectID'].union( set([trk['track']['id'] for trk in tracks_i]) )\n",
    "        sleep( pause_s )\n",
    "\n",
    "    if rvDct is not None:\n",
    "        print( \"\\n##### Get Review Data #####\" )\n",
    "        for plName_i, plID_i in rvDct.items():\n",
    "            print( plName_i, '-', plID_i, '...' )\n",
    "            tracks_i = fetch_entire_playlist_with_audio_features( plID_i )\n",
    "            assign_vectors_to_tracks( tracks_i )\n",
    "            dataDct['review'][ plName_i ] = {\n",
    "                'ID'    : plID_i,\n",
    "                'tracks': tracks_i,\n",
    "                'len'   : len( tracks_i ),\n",
    "            }\n",
    "            dataDct['reviewID'] = dataDct['reviewID'].union( set([trk['track']['id'] for trk in tracks_i]) )\n",
    "            sleep( pause_s )\n",
    "\n",
    "    print( \"\\n##### Complete #####\" )\n",
    "\n",
    "\n",
    "def analyze_db_vector_spread( db ):\n",
    "    \"\"\" Gather info for feature scaling and print it for manual scaling update \"\"\"\n",
    "    rtnMatx = np.ones( (2, _V_NUM_FEATURES,) )\n",
    "    rtnMatx[0,:] *=  1e6\n",
    "    rtnMatx[1,:] *= -1e6\n",
    "    totlDct = dict()\n",
    "    totlDct.update( db['playlists'] )\n",
    "    totlDct.update( db['review'   ] )\n",
    "    for plName_i, playls_i in totlDct.items():\n",
    "        if len( playls_i['tracks'] ):\n",
    "            matx_i = get_tracks_as_vectors( playls_i['tracks'] )\n",
    "            mMin_i = np.min( matx_i, axis = 0 )\n",
    "            mMax_i = np.max( matx_i, axis = 0 )\n",
    "            for j in range( _V_NUM_FEATURES ):\n",
    "                if mMin_i[j] < rtnMatx[0,j]:\n",
    "                    rtnMatx[0,j] = mMin_i[j]\n",
    "                if mMax_i[j] > rtnMatx[1,j]:\n",
    "                    rtnMatx[1,j] = mMax_i[j]\n",
    "    for j in range( _V_NUM_FEATURES ):\n",
    "        print( f\"Feature {j+1}, Span: {rtnMatx[1,j] - rtnMatx[0,j]}, Min: {rtnMatx[0,j]}\" )\n",
    "    return rtnMatx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9afb47f-9873-4f92-8c53-ab2438096405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Get Collection Data #####\n",
      "study01 - 0a2qoe6S7lYeZ6nlhZdA0v ...\n",
      "Stored vectors for 400 tracks!\n",
      "study02 - 6gbtR2cBq5PvkghidCvvGk ...\n",
      "Stored vectors for 400 tracks!\n",
      "study03 - 3o3lN2qntdEV7UKTuuC77K ...\n",
      "Stored vectors for 400 tracks!\n",
      "study04 - 41sFSisljvBDMBXtpp5NIw ...\n",
      "Stored vectors for 400 tracks!\n",
      "study05 - 02iS5AFGp8YVuUUqcQf8ys ...\n",
      "Stored vectors for 400 tracks!\n",
      "study06 - 6KI7A4MWrSM7EyKRUjxIi1 ...\n",
      "Stored vectors for 400 tracks!\n",
      "study07 - 3V055Md2JdrUT8tX0af7di ...\n",
      "Stored vectors for 400 tracks!\n",
      "study08 - 0tspdJlwSgiyf2O9PO6QaP ...\n",
      "Stored vectors for 400 tracks!\n",
      "study09 - 5mHRBFoQtYy2izeZ66pG95 ...\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='api.spotify.com', port=443): Read timed out. (read timeout=5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:446\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:441\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:756\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 756\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/retry.py:532\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:448\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:337\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    339\u001b[0m     )\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='api.spotify.com', port=443): Read timed out. (read timeout=5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m load_music_database( forceLoad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m )\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfetch_collection_with_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaylist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause_s\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m analyze_db_vector_spread( data );\n",
      "Cell \u001b[0;32mIn[13], line 129\u001b[0m, in \u001b[0;36mfetch_collection_with_audio_features\u001b[0;34m(dataDct, plDct, rvDct, pause_s, renewSets)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plName_i, plID_i \u001b[38;5;129;01min\u001b[39;00m plDct\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m( plName_i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, plID_i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m--> 129\u001b[0m     tracks_i \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_entire_playlist_with_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mplID_i\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     assign_vectors_to_tracks( tracks_i )\n\u001b[1;32m    131\u001b[0m     dataDct[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaylists\u001b[39m\u001b[38;5;124m'\u001b[39m][ plName_i ] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m    : plID_i,\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtracks\u001b[39m\u001b[38;5;124m'\u001b[39m: tracks_i,\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen\u001b[39m\u001b[38;5;124m'\u001b[39m   : \u001b[38;5;28mlen\u001b[39m( tracks_i ),\n\u001b[1;32m    135\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m, in \u001b[0;36mfetch_entire_playlist_with_audio_features\u001b[0;34m(playlist_ID)\u001b[0m\n\u001b[1;32m     47\u001b[0m resTracks \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     49\u001b[0m resIDs \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m resTracks]\n\u001b[0;32m---> 51\u001b[0m resFeatrs \u001b[38;5;241m=\u001b[39m \u001b[43mspot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mresIDs\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, track_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m( resTracks ):\n\u001b[1;32m     53\u001b[0m     track_i\u001b[38;5;241m.\u001b[39mupdate( resFeatrs[i] )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spotipy/client.py:1737\u001b[0m, in \u001b[0;36mSpotify.audio_features\u001b[0;34m(self, tracks)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1736\u001b[0m     tlist \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m, t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tracks]\n\u001b[0;32m-> 1737\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio-features/?ids=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtlist\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;66;03m# the response has changed, look for the new style first, and if\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# its not there, fallback on the old style\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_features\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spotipy/client.py:323\u001b[0m, in \u001b[0;36mSpotify._get\u001b[0;34m(self, url, args, payload, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    321\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(args)\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spotipy/client.py:266\u001b[0m, in \u001b[0;36mSpotify._internal_call\u001b[0;34m(self, method, url, payload, params)\u001b[0m\n\u001b[1;32m    262\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSending \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with Params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Headers: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and Body: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    263\u001b[0m              method, url, args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m), headers, args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    272\u001b[0m     results \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/adapters.py:578\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='api.spotify.com', port=443): Read timed out. (read timeout=5)"
     ]
    }
   ],
   "source": [
    "load_music_database( forceLoad = 1 )\n",
    "fetch_collection_with_audio_features( data, playlist, review, pause_s = 3.0 )\n",
    "analyze_db_vector_spread( data );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433707b-d993-49da-b535-5d83103ddcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_disallowed_entries_by_key( dct ):\n",
    "    \"\"\" Remove segments unsuitable for a genre name \"\"\"\n",
    "    blocked = [ 'soundtrack', 'for', ]\n",
    "    qKeys   = list( dct.keys() )\n",
    "    qLen    = len( qKeys )\n",
    "    delSet  = set([])\n",
    "    difFrac = 0.25\n",
    "    for i, k in enumerate( qKeys ):\n",
    "        kLo     = str(k).lower()\n",
    "        removed = False\n",
    "        # Remove short keys\n",
    "        if (len( kLo.strip() ) <= 2) and (k in dct):\n",
    "            del dct[k]\n",
    "            removed = True\n",
    "        # Remove keys with apostrophees\n",
    "        elif (\"'\" in kLo) and (k in dct):\n",
    "            del dct[k]\n",
    "            removed = True\n",
    "        # Remove keys too similar to the blocked list\n",
    "        else:\n",
    "            for blkd in blocked:\n",
    "                if ((levenshtein_dist( kLo, blkd ) / len( blkd )) < difFrac) and (k in dct):\n",
    "                    del dct[k]\n",
    "                    removed = True\n",
    "                    break\n",
    "        # If the current key is suitable, Then remove all following keys that are too similar\n",
    "        if not removed:\n",
    "            for j in range( i+1, qLen ):\n",
    "                key_j    = qKeys[j]\n",
    "                key_jLo  = str( key_j ).lower()\n",
    "                fracRght = levenshtein_dist( kLo, key_jLo ) / len( key_jLo )\n",
    "                fracLeft = levenshtein_dist( kLo, key_jLo ) / len( kLo     )\n",
    "                if (fracRght < difFrac) or (fracLeft < difFrac):\n",
    "                    if key_j in dct:\n",
    "                        del dct[ key_j ]\n",
    "                        \n",
    "\n",
    "def repair_keys( dct ):\n",
    "    \"\"\" Replace keys that have characters we don't like \"\"\"\n",
    "    badChars = [ '(', ')' ]\n",
    "    dctKeys  = list( dct.keys() )\n",
    "    for key in dctKeys:\n",
    "        nuKey = str( key )\n",
    "        p_bad = False\n",
    "        for ch in badChars:\n",
    "            if ch in key:\n",
    "                nuKey.replace( ch, '' )\n",
    "                p_bad = True\n",
    "        if p_bad and (key in dct):\n",
    "            dct[ nuKey ] = dct[ key ]\n",
    "            del dct[ key ]\n",
    "\n",
    "\n",
    "def Proper_Namify( namStr ):\n",
    "    \"\"\" Capitalize every split string, and reassemble \"\"\"\n",
    "    namSeg = str( namStr ).split()\n",
    "    Nseg   = len( namSeg )\n",
    "    rtnNam = \"\"\n",
    "    for i, seg in enumerate( namSeg ):\n",
    "        rtnNam += seg[0].upper() + seg[1:].lower()\n",
    "        if i+1 < Nseg:\n",
    "            rtnNam += ' '\n",
    "    return rtnNam\n",
    "    \n",
    "\n",
    "def extract_and_generate_genre_names( genreDct ):\n",
    "    \"\"\" Extract Spotify genre and Generate local genre \"\"\"\n",
    "    mainDist = dict()\n",
    "    loclDist = dict()\n",
    "    artSet   = set([])\n",
    "    englishD = enchant.Dict( \"en_US\" )\n",
    "    segments = list()\n",
    "    # Get artist info and local genre candidate substrings\n",
    "    for track in genreDct['tracks']:\n",
    "        # Gather artist IDs\n",
    "        for artist in track['track']['album']['artists']:\n",
    "            artSet.add( artist['id'] )\n",
    "            segments.extend( artist['name'].split() )\n",
    "        # Gather naming strings\n",
    "        segments.extend( track['track']['album']['name'].split() )\n",
    "        segments.extend( track['track']['name'].split() )\n",
    "    qSegmnts = segments[:]\n",
    "    nglshSeg = list()\n",
    "    Norig    = len( segments )\n",
    "    for qSeg in qSegmnts:\n",
    "        segments.extend( englishD.suggest( qSeg ) ) # https://stackoverflow.com/a/3789057\n",
    "    for i, seg in enumerate( segments ):\n",
    "        if i < Norig:\n",
    "            value = 1.0\n",
    "        else:\n",
    "            value = 0.5\n",
    "        if seg in loclDist:\n",
    "            loclDist[ seg ] += value\n",
    "        else:\n",
    "            loclDist[ seg ]  = value\n",
    "    \n",
    "    repair_keys( loclDist )\n",
    "    remove_disallowed_entries_by_key( loclDist )\n",
    "    # pprint( loclDist )\n",
    "\n",
    "    # Extract Spotify genre from the artist set\n",
    "    artSetLs = list( artSet )\n",
    "    Nartists = len( artSetLs )\n",
    "    artQList = list()\n",
    "    if Nartists <= _ARTIST_Q_LIM:\n",
    "        artQList.append( artSetLs )\n",
    "    else:\n",
    "        bgn = 0\n",
    "        end = 0\n",
    "        while end < Nartists:\n",
    "            bgn = end\n",
    "            end = min( end+_ARTIST_Q_LIM, Nartists )\n",
    "            artQList.append( artSetLs[bgn:end] )\n",
    "\n",
    "    for qArtLs in artQList:\n",
    "        response = spot.artists( qArtLs )\n",
    "        for artist in response['artists']:\n",
    "            for spGenre in artist['genres']:\n",
    "                if spGenre not in mainDist:\n",
    "                    mainDist[ spGenre ]  = 1\n",
    "                else:\n",
    "                    mainDist[ spGenre ] += 1\n",
    "\n",
    "    if len( mainDist ):\n",
    "        topSpGenre = sort_keys_by_value( mainDist, reverse = True )[0]\n",
    "    else:\n",
    "        topSpGenre = _NULL_GENRE\n",
    "\n",
    "    genreDct['nameSpot'] = topSpGenre\n",
    "\n",
    "    loclDist = normalize_dist( loclDist )\n",
    "    mainDist = normalize_dist( mainDist )\n",
    "    genreDct['nameDist'] = mainDist\n",
    "\n",
    "    localName = \"\"\n",
    "    namLast   = roll_outcome( mainDist )\n",
    "    if namLast is None:\n",
    "        namLast = \"Music\"\n",
    "        P_last    = 1.0\n",
    "        running   = True\n",
    "    else:\n",
    "        P_last    = mainDist[ namLast ]\n",
    "        running   = (random() < P_last)\n",
    "    \n",
    "    localName += Proper_Namify( roll_outcome( loclDist ) ) + '-' + Proper_Namify( namLast )\n",
    "\n",
    "    while running:\n",
    "        namLast = roll_outcome( loclDist )\n",
    "        P_last  = loclDist[ namLast ]\n",
    "        running = random() < P_last\n",
    "        localName += ' ' + Proper_Namify( namLast ) \n",
    "\n",
    "    genreDct['nameLocal'] = localName\n",
    "\n",
    "    print( f\"{genreDct['nameLocal']} | {genreDct['nameSpot']} | {genreDct['nameDist']}\" )\n",
    "\n",
    "\n",
    "def genre_vector_ops( gnre ):\n",
    "    \"\"\" Calculate track vectors and properties derived from them \"\"\"\n",
    "    gnre['vectors'] = get_tracks_as_vectors( gnre['tracks'] )\n",
    "    gnre['len']     = len( gnre['tracks'] )\n",
    "    if gnre['len'] > 1:\n",
    "        cntr = np.mean( gnre['vectors'], axis = 0 )\n",
    "        dim  = len( cntr )\n",
    "        for i in range( gnre['len'] ):\n",
    "            pnt_i   = gnre['vectors'][i,:]\n",
    "            dist_i  = np.linalg.norm( np.subtract( cntr, pnt_i ) )\n",
    "            alpha_i = np.exp( -dist_i )\n",
    "            cntr    = cntr * (1.0 - alpha_i) + pnt_i * alpha_i\n",
    "        gnre['center'] = cntr # 2024-08-16: This is probably guaranteed to be inside the convex hull\n",
    "        gnre['kdTree'] = cKDTree( gnre['vectors'] )\n",
    "    elif gnre['len'] > 0:\n",
    "        gnre['center'] = gnre['vectors'][0]\n",
    "        gnre['kdTree'] = cKDTree( gnre['vectors'] )\n",
    "    else:\n",
    "        gnre['center'] = None\n",
    "        gnre['kdTree'] = None\n",
    "    \n",
    "\n",
    "def generate_genres_from_track_list( tracks ):\n",
    "    \"\"\" Use DBSCAN to generate clusters based on track vectors, Give them names, Then return as a `dict` \"\"\"\n",
    "    # NOTE: This function assumes that `tracks` was built using `fetch_entire_playlist_with_audio_features`                \n",
    "\n",
    "    print( f\"\\n########## Extract genre info from {len(tracks)} tracks! ##########\\n\" )\n",
    "\n",
    "    ### Compute Clusters ###\n",
    "    trkVecs = get_tracks_as_vectors( tracks )\n",
    "    clustrs = DBSCAN( eps = _DBS_EPSILON, min_samples = _DBS_MIN_MMBR ).fit( trkVecs )\n",
    "    genres  = dict()\n",
    "    for i, trk_i in enumerate( tracks ):\n",
    "        lbl_i = clustrs.labels_[i]\n",
    "        if (lbl_i not in genres):\n",
    "            genres[ lbl_i ] = {\n",
    "                'nameSpot' : None, # --- Most prominent Spotify genre across all artists\n",
    "                'nameDist' : None, # --- Discrete distribution of Spotify genre across all artists\n",
    "                'nameLocal': None, # --- Humorous (semi-)unique name given to micro-genre\n",
    "                'tracks'   : [trk_i,], # Tracks that belong to this micro-genre\n",
    "                'len'      : 1, # ------ Number of identified tracks in the micro-genre\n",
    "                'vectors'  : None, # --- Vector representation of the tracks\n",
    "                'origins'  : list(), # Playlist(s) that tracks come from\n",
    "            }\n",
    "        else:\n",
    "            genres[ lbl_i ]['tracks'].append( trk_i )\n",
    "            genres[ lbl_i ]['len'   ] += 1\n",
    "\n",
    "    # Erase outliers\n",
    "    if -1 in genres:\n",
    "        del genres[-1]\n",
    "    print( f\"Identified {len(genres)} genres in this collection of {len(tracks)} tracks!\" )\n",
    "    # Generate unique keys for genres\n",
    "    rtnGenres = dict()\n",
    "    for k, v in genres.items():\n",
    "        rtnGenres[ str( uuid4() ) ] = v\n",
    "\n",
    "    ### Compute Center and kdTree for genre ###\n",
    "    for gnre in rtnGenres.values():\n",
    "        genre_vector_ops( gnre )\n",
    "\n",
    "        # Generate micro-genre names {Spotify, Distribution, Local}\n",
    "        extract_and_generate_genre_names( gnre )\n",
    "    \n",
    "    print( f\"\\n########## Genre extraction COMPLETE! ##########\\n\" )\n",
    "\n",
    "    return rtnGenres\n",
    "\n",
    "\n",
    "def extract_micro_genres_from_collection( dataDct ):\n",
    "    \"\"\" Extract and merge micro-genres from the entire collection defined by `dataDct` \"\"\"\n",
    "    for plName_i, playls_i in dataDct['playlists'].items():\n",
    "        print( f\"\\n### Playlist: {plName_i}, {playls_i['ID']} ###\\n\" )\n",
    "        bgn_i    = now()\n",
    "        tracks_i = playls_i['tracks']\n",
    "        gnres_i  = generate_genres_from_track_list( tracks_i )\n",
    "        for gnre_j in gnres_i.values():\n",
    "            gnre_j['origins'] = [playls_i['ID'],]\n",
    "        dataDct['genres'].update( gnres_i )\n",
    "        dur_i = now() - bgn_i\n",
    "        print( f\"\\nGenre generation from {plName_i} took {dur_i} seconds!\\n\" )\n",
    "        check_API_token() # 2024-08-21: This process can take a while on my home machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f633bf7-d7a3-4a68-8113-040c08321427",
   "metadata": {},
   "source": [
    "## Micro-Genre Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a8624-245f-4ed3-95d3-17f6f2f5966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not _SKIP_GENRE_BUILD:\n",
    "extract_micro_genres_from_collection( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ead27-3ca6-461d-be9e-e1fc49d0eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not _SKIP_GENRE_BUILD:\n",
    "#     save_music_database( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7ad60-57e7-461a-9c59-e7d1f33dc9f0",
   "metadata": {},
   "source": [
    "## Merge Micro-Genres into Mini-Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2426bd6-7333-4976-ac43-9ac506afa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_MRG_D_FACTOR =  3.0\n",
    "_MIN_GNR_MMBR = 10\n",
    "\n",
    "def merge_micro_genres_in_db( db ):\n",
    "    \"\"\" Attempt to merge similar genres in the `db` \"\"\"\n",
    "\n",
    "    print( f\"########## Attempt to merge similar genres in the music database ##########\" )\n",
    "    \n",
    "    ### Init ###\n",
    "    Ngenres = len( db['genres'] )\n",
    "    IDs     = list()\n",
    "    trees   = list()\n",
    "    pntLsts = list()\n",
    "    \n",
    "    # Gather track vectors\n",
    "    for k, v in db['genres'].items():\n",
    "        IDs.append( k )\n",
    "        trees.append( v['kdTree'] )\n",
    "        pntLsts.append( v['vectors'] )\n",
    "    \n",
    "    ### Search for merge candidates ###\n",
    "    mergeLst = list()\n",
    "    for i, gID_i in enumerate( IDs ):\n",
    "        pts_i = pntLsts[i]\n",
    "        for j in range( i+1, Ngenres ):\n",
    "            gID_j = IDs[j]\n",
    "            kdt_j = trees[j]\n",
    "            dif_j = list()\n",
    "            for pnt_k in pts_i:\n",
    "                dif_j.append( kdt_j.query( pnt_k )[0] )\n",
    "            if (np.mean( dif_j ) < (_DBS_EPSILON/_MRG_D_FACTOR)):\n",
    "                found = False\n",
    "                for mrgSet in mergeLst:\n",
    "                    if ((gID_i in mrgSet) or (gID_j in mrgSet)):\n",
    "                        mrgSet.add( gID_i )\n",
    "                        mrgSet.add( gID_j )\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    mergeLst.append( set([gID_i, gID_j,]) )\n",
    "\n",
    "    print( f\"There are {len(mergeLst)} merge jobs to perform!\" )\n",
    "    \n",
    "    ### Perform all merge jobs ###\n",
    "    for mrgJob in mergeLst:\n",
    "        mDct = {\n",
    "            'nameSpot' : None, # --- Most prominent Spotify genre across all artists\n",
    "            'nameDist' : dict(), # --- Discrete distribution of Spotify genre across all artists\n",
    "            'nameLocal': None, # --- Humorous (semi-)unique name given to micro-genre\n",
    "            'tracks'   : list(), # Tracks that belong to this micro-genre\n",
    "            'len'      : 0, # ------ Number of identified tracks in the micro-genre\n",
    "            'vectors'  : None, # --- Vector representation of the tracks\n",
    "            'kdTree'   : None, # --- Spatial tree for 'vectors'\n",
    "            'origins'  : list(), # Playlist(s) that tracks come from\n",
    "        }\n",
    "        namSplt = list()\n",
    "        keys_i = list( mrgJob )\n",
    "        print( \"\\nMerge:\" )\n",
    "        for j, key_ij in enumerate( keys_i ):\n",
    "            if key_ij in db['genres']:\n",
    "                if j > 0:\n",
    "                    print( \"\\t\\t-and-\" )\n",
    "                print( f\"\\t{db['genres'][ key_ij ]['nameLocal']}\" )\n",
    "            \n",
    "        for mID in mrgJob:\n",
    "            if mID in db['genres']:\n",
    "                # Local name components\n",
    "                namSplt.append( [db['genres'][ mID ]['len'], db['genres'][ mID ]['nameLocal'].split(),] )\n",
    "                # Tracks\n",
    "                mDct['tracks'].extend( db['genres'][ mID ]['tracks'] )\n",
    "                # Vectors\n",
    "                if mDct['vectors'] is None:\n",
    "                    mDct['vectors'] = db['genres'][ mID ]['vectors']\n",
    "                else:\n",
    "                    mDct['vectors'] = np.vstack( (mDct['vectors'], db['genres'][ mID ]['vectors'],) )\n",
    "                # Len\n",
    "                mDct['len'] += db['genres'][ mID ]['len']\n",
    "                # Origins\n",
    "                mDct['origins'].extend( db['genres'][ mID ]['origins'] )\n",
    "                # Name Distribution\n",
    "                for k, v in db['genres'][ mID ]['nameDist'].items():\n",
    "                    if k in mDct['nameDist']:\n",
    "                        mDct['nameDist'][k] += v * db['genres'][ mID ]['len']\n",
    "                    else:\n",
    "                        mDct['nameDist'][k]  = v * db['genres'][ mID ]['len']\n",
    "                # Delete the merged genre\n",
    "                del db['genres'][ mID ]\n",
    "        # Recalc the spatial tree\n",
    "        mDct['kdTree'] = cKDTree( mDct['vectors'] )\n",
    "        # Normalize Name Distribution\n",
    "        mDct['nameDist'] = normalize_dist( mDct['nameDist'] )\n",
    "        # Choose the top Spotify genre\n",
    "        if len( mDct['nameDist'] ):\n",
    "            topSpGenre = sort_keys_by_value( mDct['nameDist'], reverse = True )[0]\n",
    "        else:\n",
    "            topSpGenre = _NULL_GENRE\n",
    "        mDct['nameSpot'] = topSpGenre\n",
    "        # Construct a new local name\n",
    "        lenLst = [len( item[1] ) for item in namSplt]\n",
    "        lenMax = max( lenLst )\n",
    "        lclNam = \"\"\n",
    "        for j in range( lenMax ):\n",
    "            dice = dict()\n",
    "            for k, (mag_k, lst_k) in enumerate( namSplt ):\n",
    "                print( f\"Components: {k}, {mag_k}, {lst_k}\" )\n",
    "                len_k = len( lst_k )\n",
    "                if j < len_k:\n",
    "                    dice[ lst_k[j] ] = mag_k\n",
    "            token = roll_outcome( dice )\n",
    "            # print( namSplt )\n",
    "            # print( lenLst )\n",
    "            # pprint( dice )\n",
    "            print( f\"Token: {token}\" )\n",
    "            if j > 0:\n",
    "                lclNam += ' '\n",
    "            lclNam += (token if (token is not None) else _NULL_GENRE)\n",
    "        mDct['nameLocal'] = lclNam\n",
    "\n",
    "        # Store the merged mini-genre\n",
    "        nuID = str( uuid4() )\n",
    "        db['genres'][ nuID ] = mDct\n",
    "        print( f\"Merge Complete!: New Mini-Genre {mDct['nameLocal']} ({nuID}) created with {mDct['len']} tracks!\" )\n",
    "\n",
    "\n",
    "def move_mini_genre_outliers_to_better_homes( db ):\n",
    "    \"\"\" Attempt to rehome outlier tracks that were collected during the micro-genre creation and merge \"\"\"\n",
    "\n",
    "    ### Search for split candidates ###\n",
    "    Ngenres = len( db['genres'] )\n",
    "    IDs     = list()\n",
    "    trees   = list()\n",
    "    pntLsts = list()\n",
    "    dstLsts = list()\n",
    "\n",
    "    # Gather track vectors\n",
    "    for k, v in db['genres'].items():\n",
    "        IDs.append( k )\n",
    "        trees.append( v['kdTree'] )\n",
    "        pntLsts.append( v['vectors'] )\n",
    "\n",
    "    # Evaluate spread within each mini-genre\n",
    "    for i, pts_i in enumerate( pntLsts ):\n",
    "        avgDist = list()\n",
    "        for j, pnt_j in enumerate( pts_i ):\n",
    "            dists = list()\n",
    "            for k, pnt_k in enumerate( pts_i ):\n",
    "                dists.append( np.linalg.norm( np.subtract( pnt_j, pnt_k ) ) )\n",
    "            avgDist.append( np.mean( dists ) )\n",
    "        dstLsts.append( avgDist )\n",
    "\n",
    "    ## Evaluate relative closeness of every point in a genre to every other genre ##\n",
    "    # For every mini-genre, do\n",
    "    for i, gID_i in enumerate( IDs ):\n",
    "        pts_i = pntLsts[i][:].tolist()\n",
    "        dst_i = dstLsts[i][:]\n",
    "        # For every vector in the mini-genre, do\n",
    "        j = 0 \n",
    "        while (j < len( pts_i )):\n",
    "            pnt_j = pts_i[j]\n",
    "            dst_j = dst_i[j]\n",
    "            dMn_j = 1e6\n",
    "            gnr_j = None\n",
    "            # For every other mini-genre, Search for the shortest dist\n",
    "            for k, gID_k in enumerate( IDs ):\n",
    "                if i != k:\n",
    "                    tre_k  = db['genres'][ gID_k ]['kdTree']\n",
    "                    dst_jk = tre_k.query( pnt_j )[0]\n",
    "                    if ((dst_jk < dst_j) and (dst_jk < dMn_j)):\n",
    "                        dMn_j = dst_jk\n",
    "                        gnr_j = gID_k\n",
    "            # If a new home was found, then move\n",
    "            if gnr_j is not None:\n",
    "                print( f\"Moving track {j} of  {db['genres'][ gID_i ]['nameLocal']}  --to->  {db['genres'][ gnr_j ]['nameLocal']}\" )\n",
    "                trk_j = db['genres'][ gID_i ]['tracks'][j]\n",
    "                db['genres'][ gID_i ]['tracks'].pop(j)\n",
    "                pts_i.pop(j)\n",
    "                dst_i.pop(j)\n",
    "                db['genres'][ gnr_j ]['tracks'].append( trk_j )\n",
    "                db['genres'][ gnr_j ]['changed'] = True\n",
    "                db['genres'][ gID_i ]['changed'] = True\n",
    "            else:\n",
    "                j += 1\n",
    "                \n",
    "    # For every mini-genre, Recalc vectors if it has changed\n",
    "    for i, gID_i in enumerate( IDs ):\n",
    "        if (('changed' in db['genres'][ gID_i ]) and db['genres'][ gID_i ]['changed']):\n",
    "            gnre = db['genres'][ gID_i ]\n",
    "            genre_vector_ops( gnre )\n",
    "            gnre['changed'] = False\n",
    "\n",
    "    print( \"\\n########## Genre Report ##########\\n\" )\n",
    "    print( f\"Filter {len( db['genres'] )} to filter...\\n\" )\n",
    "    lstDel = list()\n",
    "    \n",
    "    for gID, genre in db['genres'].items():\n",
    "        if genre['len'] >= _MIN_GNR_MMBR:\n",
    "            print( f\"{genre['nameLocal']}, {genre['len']}\" )\n",
    "        else:\n",
    "            lstDel.append( gID )\n",
    "\n",
    "    print( \"Deleting ...\", end = \" \" )\n",
    "    for gID in lstDel:\n",
    "        print( gID, end = \", \" )\n",
    "        del db['genres'][ gID ]\n",
    "        \n",
    "    print( f\"\\nDeleted {len( lstDel )} mini-genres, {len( db['genres'] )} remain\" )\n",
    "    print( \"\\n########## Report Complete ##########\\n\" )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd7de1-672b-40a8-a8eb-ea0b0b47e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_micro_genres_in_db( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59681a-a812-48e1-bca7-3728efa29304",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_mini_genre_outliers_to_better_homes( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275c9fe-19f3-4863-93a4-fe4fa917e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_music_database( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799d31b-8ab0-4e1d-95c3-888f18570ffb",
   "metadata": {},
   "source": [
    "# Search Version 02, Graded by Mini-Genre Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df6fd6-f593-4db9-8041-f50391de4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NU_REL_Q_KEY = \"NewReleases:Albums\" # Query Key for New Releases\n",
    "_FEAT_PL_Q_KY = \"Featured:Playlists\" # Query Key for Featured Playlists\n",
    "_ART_Q_PREFIX = \"ArtistTopTracks:\" # - Query Key Prefix for Artist Top Tracks\n",
    "\n",
    "\n",
    "def get_tracks_from_new_releases( N, div = 12, db = None, pause_s = 0.25 ):\n",
    "    \"\"\" Get `N` tracks from newly-released albums \"\"\"\n",
    "    totTrks = list()\n",
    "    Nalbums = int( ceil( N / div ) )\n",
    "    albOfst = 0\n",
    "    count   = 0\n",
    "\n",
    "    while count < N: \n",
    "    \n",
    "        if db is not None:\n",
    "            if _NU_REL_Q_KEY in db['queries']:\n",
    "                albOfst = db['queries'][ _NU_REL_Q_KEY ]\n",
    "                db['queries'][ _NU_REL_Q_KEY ] += Nalbums\n",
    "            else:\n",
    "                db['queries'][ _NU_REL_Q_KEY ] = Nalbums\n",
    "        \n",
    "        response = spot.new_releases( limit = Nalbums, offset = albOfst )\n",
    "        nuAlbums = [item['id'] for item in response['albums']['items']]\n",
    "        pause( pause_s )\n",
    "        \n",
    "        for albumID in nuAlbums:\n",
    "            res    = spot.album_tracks( albumID, limit = 50, offset = 0 )\n",
    "            tracks = res['items']\n",
    "            totTrks.extend( tracks )\n",
    "            count += len( tracks )\n",
    "            pause( pause_s )\n",
    "\n",
    "    return totTrks\n",
    "\n",
    "\n",
    "def get_recommended_tracks_from_db( db, N_tracks, pause_s = 0.25 ):\n",
    "    \"\"\" Recommended tracks by both artists and genres \"\"\"\n",
    "    count  = 0\n",
    "    gIDs   = list( db['genres'].keys() )\n",
    "    rtnLst = list()\n",
    "    while count < N_tracks:\n",
    "        recArt = list()\n",
    "        recTrk = list()\n",
    "        recGnr = list()\n",
    "        gID    = choice( gIDs )\n",
    "        gnre   = db['genres'][ gID ]\n",
    "        for i in range( 5 ):\n",
    "            trk_i = choice( gnre['tracks'] )\n",
    "            art_i = choice( trk_i['track']['album']['artists'] )\n",
    "            if i % 3 == 0:\n",
    "                recArt.append( art_i['id'] )\n",
    "            elif i % 2 == 0:    \n",
    "                recTrk.append( trk_i['track']['id'] )\n",
    "            else:\n",
    "                recGnr.append( roll_outcome( gnre['nameDist'] ) )\n",
    "        response = spot.recommendations(\n",
    "            seed_artists = recArt, \n",
    "            seed_genres  = recGnr, \n",
    "            seed_tracks  = recTrk, \n",
    "            limit        = min( _RESPONSE_LIMIT, N_tracks - count )\n",
    "        )\n",
    "        pause( pause_s )\n",
    "        # pprint( response )\n",
    "        tracks = response['tracks']\n",
    "        count += len( tracks )\n",
    "        rtnLst.extend( tracks )\n",
    "    return rtnLst\n",
    "        \n",
    "\n",
    "def get_tracks_from_related_artist( db, N_tracks, pause_s = 0.25 ):\n",
    "    \"\"\" Attempt to get fresh tracks from Spotify given artists currently in the collection \"\"\"\n",
    "    skipN  = 5\n",
    "    count  = 0\n",
    "    plNam  = list( db['playlists'].keys() )\n",
    "    rtnLst = list()\n",
    "\n",
    "    while count < N_tracks:\n",
    "        tracks = db['playlists'][ choice( plNam ) ]['tracks']\n",
    "        artist = choice( choice( tracks )['track']['album']['artists'] )['id']\n",
    "        res    = spot.artist_related_artists( artist )\n",
    "        artLst = [item['id'] for item in res['artists']]\n",
    "        Nart   = len( artLst )\n",
    "        pause( pause_s )\n",
    "\n",
    "        # For each artist, Get top tracks\n",
    "        for art_i in artLst:\n",
    "            qArtist = _ART_Q_PREFIX + str( art_i )\n",
    "            if qArtist not in db['queries']:\n",
    "                res = spot.artist_top_tracks( art_i )\n",
    "                pause( pause_s )\n",
    "                count += len( res['tracks'] )\n",
    "                rtnLst.extend( res['tracks'] )\n",
    "                db['queries'][ qArtist ] = 1\n",
    "\n",
    "        # For each group of artists, Get recommendations\n",
    "        bgn  = 0\n",
    "        end  = 0\n",
    "        grps = list()\n",
    "        while end < Nart:\n",
    "            bgn = end\n",
    "            end = min( end+skipN, Nart )\n",
    "            grps.append( artLst[ bgn:end ] )\n",
    "        for artGrp in grps:\n",
    "            response = spot.recommendations(\n",
    "                seed_artists = artGrp, \n",
    "                limit        = min( _RESPONSE_LIMIT, N_tracks - count )\n",
    "            )\n",
    "            pause( pause_s )\n",
    "            # pprint( response )\n",
    "            tracks = response['tracks']\n",
    "            count += len( tracks )\n",
    "            rtnLst.extend( tracks )\n",
    "\n",
    "    return rtnLst\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dac839-494c-419e-89f6-7afe2b3aad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks = get_recommended_tracks_from_db( data, 100 )\n",
    "# print( len( tracks ) )\n",
    "# get_tracks_from_related_artist( data, 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483804f4-cd20-4b5e-8065-4f8c6f12594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint( tracks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba0af0-c2dc-4b45-bb88-1389c39509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44ab3a-ae7b-40c6-ae9d-665ffe05ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trkData = get_tracks_as_vectors( tracks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b195f12-f033-4bee-b23e-5a8e8839a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA( n_components = 10 )\n",
    "\n",
    "# pca.fit( trkData )\n",
    "# print( pca.explained_variance_ )\n",
    "# for comp in pca.components_:\n",
    "#     comp_i = np.abs( comp )\n",
    "#     print( np.argmax( comp_i ) )\n",
    "# print( pca.get_params()          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8984ce-93d0-4dfd-8152-3db84691a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print( len(  ) )\n",
    "# print( np.max( clustering.labels_ ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a20f0f-d852-47d8-ad85-e768c66940bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
